{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_loc = \"Path/to/workspace/\"\n",
    "root = \"Path/to/Data/\"\n",
    "save_root = root + \"/Step_1_data/Dataset_netrin_ST_all_spot/\" \n",
    "import sys\n",
    "sys.path.append(code_loc ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a vocab list\n",
    "# this related with the gene2vec model\n",
    "#----> pre-trained part \n",
    "vocab_loc = code_loc +\"/support_data/vocab_gene2vec_16906.pkl\"\n",
    "target_label = 'Ground_Truth' # the label to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = root+\"/Step_0_data/2024_3_27_netrin_spatial_expression_data_unnormalised_allspots.csv\"  #/home/shi/WorkSpace/projects/scMultiNet_Data/Step_0_data/2024_3_27_netrin_spatial_expression_data_unnormalised_allspots.csv\n",
    "data_path = root +  \"/Step_0_data/2024_03_27_EMT_netrin_ST_all_spots.h5ad\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SAMD11</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>PLEKHN1</th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>AGRN</th>\n",
       "      <th>C1orf159</th>\n",
       "      <th>TNFRSF18</th>\n",
       "      <th>...</th>\n",
       "      <th>DKC1</th>\n",
       "      <th>MPP1</th>\n",
       "      <th>FUNDC2</th>\n",
       "      <th>CMC4</th>\n",
       "      <th>MTCP1</th>\n",
       "      <th>VBP1</th>\n",
       "      <th>CLIC2</th>\n",
       "      <th>TMLHE</th>\n",
       "      <th>SPRY3</th>\n",
       "      <th>VAMP7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAACTGCTGGCTCCAA-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAGACCCAAGTCGCG-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAGGGATGTAGCAAG-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAGTCACTGATGTAA-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAATACCTATAAGCAT-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>TTGTTAGCAAATTCGA-1_3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>TTGTTCAGTGTGCTAC-1_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>TTGTTGTGTGTCAAGA-1_3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>TTGTTTCATTAGTCTA-1_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>TTGTTTGTGTAAATTC-1_3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4283 rows × 12135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unnamed: 0  SAMD11  NOC2L  KLHL17  PLEKHN1  HES4  ISG15  AGRN  \\\n",
       "0     AAACTGCTGGCTCCAA-1_0     0.0    0.0     0.0      0.0   0.0    0.0   0.0   \n",
       "1     AAAGACCCAAGTCGCG-1_0     0.0    0.0     0.0      1.0   0.0    0.0   0.0   \n",
       "2     AAAGGGATGTAGCAAG-1_0     0.0    0.0     0.0      0.0   0.0    0.0   2.0   \n",
       "3     AAAGTCACTGATGTAA-1_0     0.0    0.0     0.0      1.0   1.0    0.0   0.0   \n",
       "4     AAATACCTATAAGCAT-1_0     0.0    0.0     0.0      0.0   1.0    0.0   1.0   \n",
       "...                    ...     ...    ...     ...      ...   ...    ...   ...   \n",
       "4278  TTGTTAGCAAATTCGA-1_3     3.0    5.0     0.0      1.0  11.0    0.0  13.0   \n",
       "4279  TTGTTCAGTGTGCTAC-1_3     0.0    3.0     0.0      0.0  20.0    1.0  13.0   \n",
       "4280  TTGTTGTGTGTCAAGA-1_3     2.0    4.0     0.0      0.0   6.0    4.0  10.0   \n",
       "4281  TTGTTTCATTAGTCTA-1_3     4.0    5.0     2.0      0.0   4.0    1.0   6.0   \n",
       "4282  TTGTTTGTGTAAATTC-1_3     2.0    7.0     1.0      0.0  29.0    4.0  11.0   \n",
       "\n",
       "      C1orf159  TNFRSF18  ...  DKC1  MPP1  FUNDC2  CMC4  MTCP1  VBP1  CLIC2  \\\n",
       "0          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   1.0    0.0   \n",
       "1          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "2          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "3          1.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "4          1.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "...        ...       ...  ...   ...   ...     ...   ...    ...   ...    ...   \n",
       "4278       0.0       0.0  ...   2.0   0.0     1.0   0.0    0.0   2.0    0.0   \n",
       "4279       0.0       0.0  ...  11.0   0.0     1.0   6.0    0.0   6.0    0.0   \n",
       "4280       0.0       0.0  ...   3.0   0.0     2.0   6.0    0.0   4.0    0.0   \n",
       "4281       0.0       0.0  ...   2.0   0.0     1.0   2.0    0.0   4.0    0.0   \n",
       "4282       1.0       0.0  ...   9.0   0.0     2.0   2.0    0.0   5.0    0.0   \n",
       "\n",
       "      TMLHE  SPRY3  VAMP7  \n",
       "0       0.0    0.0    0.0  \n",
       "1       0.0    0.0    0.0  \n",
       "2       1.0    0.0    0.0  \n",
       "3       0.0    0.0    0.0  \n",
       "4       0.0    0.0    1.0  \n",
       "...     ...    ...    ...  \n",
       "4278    0.0    0.0    1.0  \n",
       "4279    3.0    0.0    3.0  \n",
       "4280    1.0    2.0    1.0  \n",
       "4281    0.0    0.0    1.0  \n",
       "4282    2.0    0.0    6.0  \n",
       "\n",
       "[4283 rows x 12135 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4283, 12134) 4283 12134\n",
      "False False\n",
      "0.0 3518.0\n",
      "(4283, 12134)\n",
      "AnnData object with n_obs × n_vars = 4283 × 12134\n",
      "    obs: 'Ground_Truth'                       Ground_Truth\n",
      "AAACTGCTGGCTCCAA-1_0             0\n",
      "AAAGACCCAAGTCGCG-1_0             1\n",
      "AAAGGGATGTAGCAAG-1_0             0\n",
      "AAAGTCACTGATGTAA-1_0             0\n",
      "AAATACCTATAAGCAT-1_0             0\n",
      "...                            ...\n",
      "TTGTTAGCAAATTCGA-1_3             1\n",
      "TTGTTCAGTGTGCTAC-1_3             0\n",
      "TTGTTGTGTGTCAAGA-1_3             1\n",
      "TTGTTTCATTAGTCTA-1_3             0\n",
      "TTGTTTGTGTAAATTC-1_3             1\n",
      "\n",
      "[4283 rows x 1 columns] Empty DataFrame\n",
      "Columns: []\n",
      "Index: [SAMD11, NOC2L, KLHL17, PLEKHN1, HES4, ISG15, AGRN, C1orf159, TNFRSF18, TNFRSF4, SDF4, B3GALT6, UBE2J2, SCNN1D, ACAP3, PUSL1, INTS11, CPTP, DVL1, MXRA8, AURKAIP1, CCNL2, ANKRD65, VWA1, ATAD3C, TMEM240, SSU72, FNDC10, MIB2, MMP23B, CDK11B, SLC35E2B, SLC35E2A, NADK, GNB1, CALML6, PRKCZ, FAAP20, SKI, MORN1, RER1, PEX10, PLCH2, PANK4, TNFRSF14, PRXL2B, ARHGEF16, MEGF6, TPRG1L, WRAP73, SMIM1, LRRC47, CEP104, DFFB, C1orf174, AJAP1, NPHP4, KCNAB2, RNF207, ICMT, GPR153, ACOT7, HES2, ESPN, TNFRSF25, PLEKHG5, NOL9, ZBTB48, KLHL21, PHF13, THAP3, DNAJC11, CAMTA1, VAMP3, PER3, PARK7, ERRFI1, SLC45A1, RERE, ENO1, SLC2A5, GPR157, H6PD, SPSB1, TMEM201, PIK3CD, CLSTN1, CTNNBIP1, LZIC, NMNAT1, RBP7, UBE4B, KIF1B, PGD, CENPS, DFFA, PEX14, CASZ1, SRM, EXOSC10, ...]\n",
      "\n",
      "[12134 rows x 0 columns] [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.02842524 0.         0.        ]\n",
      " ...\n",
      " [0.05685048 0.11370097 0.         ... 0.02842524 0.05685048 0.02842524]\n",
      " [0.11370097 0.14212621 0.05685048 ... 0.         0.         0.02842524]\n",
      " [0.05685048 0.19897669 0.02842524 ... 0.05685048 0.         0.17055145]]\n"
     ]
    }
   ],
   "source": [
    "sample_id = df.iloc[:,0].values\n",
    "gene_id = df.columns[1:].values\n",
    "data_x = df.iloc[:,1:].values\n",
    "\n",
    "print(data_x.shape, len(sample_id), len(gene_id))\n",
    "\n",
    "import numpy as np\n",
    "# confirm nan and inf in data_x\n",
    "print(np.isnan(data_x).any(), np.isinf(data_x).any())\n",
    "# set nan to 0\n",
    "data_x = np.nan_to_num(data_x)\n",
    "\n",
    "# normalize the data to 0-100\n",
    "# 找到数据的最小值和最大值\n",
    "min_val = np.min(data_x)\n",
    "max_val = np.max(data_x)\n",
    "print(min_val, max_val )\n",
    "# 将数据归一化到 0 到 100 的范围\n",
    "normalized_data_x = 100 * (data_x - min_val) / (max_val - min_val)\n",
    "print(normalized_data_x.shape)\n",
    "# create the adata matrix\n",
    "import anndata\n",
    "adata = anndata.AnnData(X = normalized_data_x, obs = pd.DataFrame(index = sample_id), var = pd.DataFrame(index = gene_id))\n",
    "adata.obs[\"Ground_Truth\"] = np.random.choice([0,1], adata.shape[0])\n",
    "print(adata, adata.obs, adata.var, adata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata.write(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 4283 × 12134\n",
      "    obs: 'Ground_Truth'\n",
      "Index(['AAACTGCTGGCTCCAA-1_0', 'AAAGACCCAAGTCGCG-1_0', 'AAAGGGATGTAGCAAG-1_0',\n",
      "       'AAAGTCACTGATGTAA-1_0', 'AAATACCTATAAGCAT-1_0', 'AAATAGGGTGCTATTG-1_0',\n",
      "       'AAATCCGATACACGCC-1_0', 'AAATTAACGGGTAGCT-1_0', 'AACCTCGCTTTAGCCC-1_0',\n",
      "       'AACGATAATGCCGTAG-1_0',\n",
      "       ...\n",
      "       'TTGTAATCCGTACTCG-1_3', 'TTGTATCACACAGAAT-1_3', 'TTGTCGTTCAGTTACC-1_3',\n",
      "       'TTGTGGCCCTGACAGT-1_3', 'TTGTGGTAGGAGGGAT-1_3', 'TTGTTAGCAAATTCGA-1_3',\n",
      "       'TTGTTCAGTGTGCTAC-1_3', 'TTGTTGTGTGTCAAGA-1_3', 'TTGTTTCATTAGTCTA-1_3',\n",
      "       'TTGTTTGTGTAAATTC-1_3'],\n",
      "      dtype='object', length=4283)\n",
      "Index(['SAMD11', 'NOC2L', 'KLHL17', 'PLEKHN1', 'HES4', 'ISG15', 'AGRN',\n",
      "       'C1orf159', 'TNFRSF18', 'TNFRSF4',\n",
      "       ...\n",
      "       'DKC1', 'MPP1', 'FUNDC2', 'CMC4', 'MTCP1', 'VBP1', 'CLIC2', 'TMLHE',\n",
      "       'SPRY3', 'VAMP7'],\n",
      "      dtype='object', length=12134)\n",
      " negative values: 0\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "adata = sc.read(data_path)\n",
    "\n",
    "print(adata)\n",
    "print(adata.obs.index)\n",
    "print(adata.var.index)\n",
    "print(f\" negative values: {np.sum(adata.X<0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in adata.X: 0 with shape (4283, 12134)\n"
     ]
    }
   ],
   "source": [
    "# adata.X 有多少nan\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# 假设 adata.X 是你的稀疏矩阵\n",
    "# 首先，检查它是否为稀疏矩阵\n",
    "if issparse(adata.X):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.isnan(adata.X.toarray()).sum()\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.isnan(adata.X).sum()\n",
    "\n",
    "print(f\"NaN values in adata.X: {nan_count} with shape {adata.X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_para(var_idx=None, obs_idx='Ground_Truth', vocab_loc='/home/shi/WorkSpace/projects/scMultiNet_workspace//Experiment/support_data/vocab_16k.json', gene_vocab=None, use_key='X', filter_gene_by_counts=False, filter_cell_by_counts=0, normalize_total=10000.0, result_normed_key='X_normed', log1p=True, result_log1p_key='X_log1p', log1p_base=2, subset_hvg=False, hvg_use_key=None, hvg_flavor='seurat_v3', binning=None, result_binned_key='X_binned', tokenize_name='scBERT', return_pt=True, append_cls=True, include_zero_gene=False, cls_token='<cls>', max_len=16000, pad_token='<pad>', pad_value=-2, cls_appended=True, mask_ratio=0.15, mask_value=-1, preprocessed_loc=None, data_layer_name='X_log1p', label_key='Ground_Truth', batch_label_key=None, cls_nb=5, binarize=None, bins=None, bin_min=None, bin_max=None, save_in_obs=True, auto_map_str_labels=True, map_dict=None, n_splits=1, test_size=None, random_state=2023, shuffle=True, sort_seq_batch=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scLLM.Dataset.paras import Dataset_para\n",
    "# define pre-processing by follow original implementation of scBERT\n",
    "\n",
    "from scLLM.Dataset.paras import Dataset_para\n",
    "# define pre-processing by follow original implementation of scBERT \n",
    "dataset_para_cls = Dataset_para(\n",
    "                            var_idx=None,\n",
    "                            obs_idx=\"Ground_Truth\",\n",
    "                            vocab_loc=code_loc + \"/Experiment/support_data/vocab_16k.json\",\n",
    "                            filter_gene_by_counts=False,\n",
    "                            filter_cell_by_counts=0,\n",
    "                            log1p=True,\n",
    "                            log1p_base=2,\n",
    "\n",
    "                            #\n",
    "                            tokenize_name=\"scBERT\",\n",
    "                            cls_nb=5,\n",
    "                            data_layer_name=\"X_log1p\",\n",
    "                            label_key = target_label,#\"Ground_truth\",#\"Ground_truth\",\n",
    "\n",
    "                            test_size=None,#0.2, #use all data to inference\n",
    "                            binarize=None, # not binarize use original label\n",
    "\n",
    "                            )\n",
    "\n",
    "dataset_para_reg = Dataset_para(\n",
    "        vocab_loc=code_loc +\"/Experiment/support_data/vocab_16k.json\",\n",
    "        var_idx = None,#\"genes.gene_short_name\",\n",
    "        obs_idx=\"pseudotimes\",\n",
    "        filter_gene_by_counts=False,\n",
    "        filter_cell_by_counts=200,\n",
    "        log1p=True,\n",
    "        log1p_base=2,\n",
    "\n",
    "        tokenize_name=\"scBERT\",\n",
    "        cls_nb=1,\n",
    "        data_layer_name=\"X_log1p\",\n",
    "\n",
    "        auto_map_str_labels=False,\n",
    "        label_key = target_label,#\"pseudotimes\",\n",
    "\n",
    "        test_size=None,#0.2, #use all data to inference\n",
    "        binarize=None, # not binarize use original label for regression\n",
    "    )\n",
    "\n",
    "dataset_para = dataset_para_cls#dataset_para_cls\n",
    "print(dataset_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scLLM - INFO - Initializing preprocessor ...\n",
      "scLLM - INFO - use default vocab from dataset_para\n",
      "scLLM - INFO - load vocab from /home/shi/WorkSpace/projects/scMultiNet_workspace//Experiment/support_data/vocab_16k.json\n",
      "scLLM - INFO - Load data from anndata object.\n",
      "scLLM - DEBUG - In original adata with gene 12134\n",
      "scLLM - DEBUG - In original adata with gene 12134\n",
      "scLLM - DEBUG - processing 0/16906\n",
      "scLLM - DEBUG - processing 2000/16906\n",
      "scLLM - DEBUG - processing 4000/16906\n",
      "scLLM - DEBUG - processing 6000/16906\n",
      "scLLM - DEBUG - processing 8000/16906\n",
      "scLLM - DEBUG - processing 10000/16906\n",
      "scLLM - DEBUG - processing 12000/16906\n",
      "scLLM - DEBUG - processing 14000/16906\n",
      "scLLM - DEBUG - processing 16000/16906\n",
      "scLLM - INFO - create anndata in scLLM format..\n",
      "scLLM - DEBUG - restore anndata in scLLM format..\n",
      "scLLM - INFO - Done.\n",
      "scLLM - INFO - Preprocessing data with shape: (4283, 16906) ...\n",
      "scLLM - INFO - Filtering cells by counts ...\n",
      "scLLM - INFO - Filtered cells: 4283\n",
      "scLLM - INFO - Normalizing total counts ...\n",
      "scLLM - INFO - Log1p transforming ...\n",
      "scLLM - INFO - Preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "# if this is the first time to run, need this block to init translate=True\n",
    "# init preprocessor\n",
    "from scLLM.Dataset.Reader import scReader\n",
    "data_reader = scReader(dataset_para=dataset_para)\n",
    "# init vocab\n",
    "data_reader.init_vocab()\n",
    "\n",
    "# load data\n",
    "data_reader.load_adata(loc = data_path,translate=True)\n",
    "\n",
    "## preprocess\n",
    "data_reader.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 4283 × 16906\n",
      "    obs: 'Ground_Truth', 'n_counts'\n",
      "    uns: 'log1p'\n",
      "    layers: 'X_normed', 'X_log1p' (4283, 16906)\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.adata, data_reader.adata.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in adata.X: 0\n"
     ]
    }
   ],
   "source": [
    "# get layer X_log1p\n",
    "data = data_reader.adata.layers[\"X_log1p\"] \n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "# set nan to 0\n",
    "if isinstance(data, csr_matrix):\n",
    "    # 使用 CSR 格式的稀疏矩阵可以直接通过 data.data 访问非零元素\n",
    "    data.data[np.isnan(data.data)] = 0\n",
    "    # 注意: 这里没有改变矩阵的稀疏结构，只是将非零元素中的 NaN 值替换为 0\n",
    "    data.data[np.isinf(data.data)] = data.data.max()\n",
    "else:\n",
    "    # 如果 data 不是稀疏矩阵，可以直接使用 numpy 的方法\n",
    "    data[np.isnan(data)] = 0\n",
    "    data[np.isinf(data)] = data.max()\n",
    "\n",
    "if issparse(data):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.isnan(data.toarray()).sum()\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.isnan(data).sum()\n",
    "\n",
    "print(f\"NaN values in adata.X: {nan_count}\")\n",
    "data_reader.adata.layers[\"X_log1p\"]  = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values in adata.X: 0\n"
     ]
    }
   ],
   "source": [
    "# data <0 的个数\n",
    "if issparse(data):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.sum(data.toarray()<0)\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.sum(data<0)\n",
    "\n",
    "print(f\"Negative values in adata.X: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4283, 16906)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "label_unique = data_reader.adata.obs[dataset_para.label_key].unique()\n",
    "print(label_unique)\n",
    "label_dict = {'fake_lable_0':0,'fake_lable_1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fake label\n",
    "#data_reader.adata.obs[dataset_para.label_key]\n",
    "revert_dict = {v:k for k,v in label_dict.items()}\n",
    "# map to str label\n",
    "data_reader.adata.obs[dataset_para.label_key] = data_reader.adata.obs[dataset_para.label_key].map(revert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scLLM - INFO - Map string labels to int automatically.\n",
      "scLLM - INFO - Mapping from {'fake_lable_0': 0, 'fake_lable_1': 1}\n",
      "scLLM - INFO - Discritize label Ground_Truth in obs_names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size:  4283\n",
      "no valset\n",
      "{'fake_lable_0': 0, 'fake_lable_1': 1}\n",
      "weights:  None\n"
     ]
    }
   ],
   "source": [
    "trainset,valset,weights = data_reader.postprocess()\n",
    "\n",
    "# 输出数据集信息\n",
    "print(\"trainset size: \",len(trainset))\n",
    "print(\"valset size: \",len(valset)) if valset is not None else print(\"no valset\")\n",
    "print(label_dict)\n",
    "print(\"weights: \",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.sample_id = data_reader.adata.obs.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4283 4283 (4283, 16906)\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset.sample_id), len(trainset.label), trainset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "# 为trainset 添加其他labels\n",
    "\n",
    "#target_task = f\"/TrVal_dataset_GT_{target_stimulate}_{target_cellline}.pkl\"\n",
    "target_task = f\"/TrVal_dataset_{target_label}.pkl\"\n",
    "loc = save_root + target_task\n",
    "# 保存 trainset 到文件，并关联相应labels\n",
    "with open(loc,\"wb\") as f:\n",
    "    dill.dump([trainset,valset,weights,label_dict],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
